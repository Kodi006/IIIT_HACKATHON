# -*- coding: utf-8 -*-
"""Medical_QA_Assistant_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NxcvlbIDlUJubtvopc5UUed1Os41D4Mq
"""


import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model

dataset = load_dataset("medmcqa")

# Use a subset to keep training fast for hackathon
train_data = dataset["train"].shuffle(seed=42).select(range(5000))
val_data = dataset["validation"].shuffle(seed=42).select(range(500))

model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto"
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

def preprocess(example):
    question = example["question"]
    options = f"""
A. {example['opa']}
B. {example['opb']}
C. {example['opc']}
D. {example['opd']}
"""
    answer_map = ["A", "B", "C", "D"]
    correct = answer_map[example["cop"]]

    instruction = (
        "Answer the medical question and explain briefly.\n\n"
        f"Question: {question}\n"
        f"Options: {options}"
    )

    response = (
        f"The correct answer is option {correct}. "
        f"This option best explains the medical condition described."
    )

    model_inputs = tokenizer(
        instruction,
        truncation=True,
        padding="longest"
    )

    labels = tokenizer(
        response,
        truncation=True,
        padding="longest"
    )

    labels["input_ids"] = [
        (t if t != tokenizer.pad_token_id else -100)
        for t in labels["input_ids"]
    ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_tokenized = train_data.map(
    preprocess,
    remove_columns=train_data.column_names
)

val_tokenized = val_data.map(
    preprocess,
    remove_columns=val_data.column_names
)

training_args = TrainingArguments(
    output_dir="./medical_assistant",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=1e-4,   # IMPORTANT
    num_train_epochs=5,
    fp16=True,
    logging_steps=20,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=data_collator
)

trainer.train()


def chat():
    print("Medical Assistant Ready (type 'exit' to stop)\n")
    while True:
        user_input = input("User: ")
        if user_input.lower() == "exit":
            break

        prompt = f"Answer the medical question clearly and safely:\n{user_input}"

        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            do_sample=True,
            temperature=0.7
        )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("Assistant:", response, "\n")

chat()