{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• Advanced Medical AI Assistant (Fixed)\n",
                "\n",
                "This notebook contains the **fixed** pipeline for running the Medical AI Assistant on Google Colab with GPU support.\n",
                "\n",
                "## Instructions\n",
                "1. **Runtime**: Ensure you are using a GPU Runtime (Runtime > Change runtime type > T4 GPU).\n",
                "2. **Configuration**: Enter your HuggingFace and Ngrok tokens in **Cell 3**.\n",
                "3. **Run All**: Run the cells sequentially."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 1: Install Dependencies ---\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "print(\"Installing dependencies... (This may take a few minutes)\")\n",
                "packages = [\n",
                "    \"requests==2.32.3\", \"torch\", \"transformers\", \"peft\", \"bitsandbytes\", \"trl\", \"accelerate\",\n",
                "    \"datasets\", \"langchain\", \"langchain-community\", \"langchain-huggingface\", \"chromadb\",\n",
                "    \"sentence-transformers\", \"gradio\", \"tiktoken\", \"pypdf\", \"scipy\", \"numpy\", \"huggingface_hub\",\n",
                "    \"fastapi\", \"uvicorn\", \"pyngrok\", \"nest-asyncio\", \"python-multipart\"\n",
                "]\n",
                "# Install silently to avoid clutter\n",
                "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n",
                "print(\"‚úÖ Dependencies installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 2: Imports ---\n",
                "import os\n",
                "import gc\n",
                "import torch\n",
                "import asyncio\n",
                "import uvicorn\n",
                "import nest_asyncio\n",
                "from pyngrok import ngrok\n",
                "from fastapi import FastAPI, UploadFile, File, Form\n",
                "from pydantic import BaseModel\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
                ")\n",
                "from peft import PeftModel\n",
                "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
                "from langchain_community.vectorstores import Chroma\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.chains import RetrievalQA\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Apply nest_asyncio to allow uvicorn to run in Jupyter/Colab environment\n",
                "nest_asyncio.apply()\n",
                "print(\"‚úÖ Imports successful.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 3: Configuration & Auth ---\n",
                "# ‚ö†Ô∏è REPLACE WITH YOUR TOKENS\n",
                "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\" \n",
                "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"\n",
                "\n",
                "# Model Config\n",
                "BASE_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
                "ADAPTER_NAME = \"medical_assistant_adapter\"\n",
                "CHROMA_DB_DIR = \"./chroma_db\"\n",
                "\n",
                "if HF_TOKEN != \"YOUR_HUGGINGFACE_TOKEN_HERE\":\n",
                "    login(token=HF_TOKEN)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è WARNING: You haven't set your HuggingFace token. Some models might fail.\")\n",
                "\n",
                "if NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n",
                "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è WARNING: Ngrok Token not set. Public URL will fail.\")\n",
                "\n",
                "print(\"‚úÖ Configuration set.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 4: Logic / RAG Class ---\n",
                "class MedicalRAG:\n",
                "    def __init__(self, persist_dir=CHROMA_DB_DIR):\n",
                "        self.persist_dir = persist_dir\n",
                "        # Lightweight embeddings model suitable for CPU/Colab\n",
                "        self.embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "        self.vectordb = None\n",
                "\n",
                "    def ingest_documents(self, file_paths):\n",
                "        \"\"\"Ingests medical documents (PDFs or Text) into the vector store.\"\"\"\n",
                "        docs = []\n",
                "        for path in file_paths:\n",
                "            print(f\"Loading {path}...\")\n",
                "            try:\n",
                "                if path.endswith(\".pdf\"):\n",
                "                    loader = PyPDFLoader(path)\n",
                "                    docs.extend(loader.load())\n",
                "                elif path.endswith(\".txt\"):\n",
                "                    loader = TextLoader(path)\n",
                "                    docs.extend(loader.load())\n",
                "            except Exception as e:\n",
                "                print(f\"Error loading {path}: {e}\")\n",
                "\n",
                "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "        splits = text_splitter.split_documents(docs)\n",
                "\n",
                "        if not splits:\n",
                "            print(\"No text found in documents.\")\n",
                "            return\n",
                "\n",
                "        print(f\"Creating vector store with {len(splits)} chunks...\")\n",
                "        # Clean up existing DB to start fresh for demo purposes\n",
                "        if os.path.exists(self.persist_dir):\n",
                "             import shutil\n",
                "             shutil.rmtree(self.persist_dir)\n",
                "\n",
                "        self.vectordb = Chroma.from_documents(\n",
                "            documents=splits,\n",
                "            embedding=self.embedding_function,\n",
                "            persist_directory=self.persist_dir\n",
                "        )\n",
                "        print(\"Vector store created and saved.\")\n",
                "\n",
                "    def load_vector_store(self):\n",
                "        if os.path.exists(self.persist_dir):\n",
                "            self.vectordb = Chroma(persist_directory=self.persist_dir, embedding_function=self.embedding_function)\n",
                "            print(\"Loaded existing vector store.\")\n",
                "            return True\n",
                "        return False\n",
                "\n",
                "    def setup_rag_pipeline(self, model, tokenizer):\n",
                "        \"\"\"Sets up the RAG chain using the loaded model.\"\"\"\n",
                "        if not self.vectordb:\n",
                "            self.load_vector_store()\n",
                "\n",
                "        pipe = pipeline(\n",
                "            \"text-generation\",\n",
                "            model=model,\n",
                "            tokenizer=tokenizer,\n",
                "            max_new_tokens=512,\n",
                "            temperature=0.7,\n",
                "            top_p=0.95,\n",
                "            repetition_penalty=1.15\n",
                "        )\n",
                "\n",
                "        llm = HuggingFacePipeline(pipeline=pipe)\n",
                "\n",
                "        template = \"\"\"<|im_start|>system\n",
                "You are an advanced medical assistant. Use the following pieces of context to answer the user's question.\n",
                "If the answer is not in the context, say you don't know, but try to be helpful based on general medical knowledge.\n",
                "Always prioritize patient safety.\n",
                "Context: {context}<|im_end|>\n",
                "<|im_start|>user\n",
                "{question}<|im_end|>\n",
                "<|im_start|>assistant\n",
                "\"\"\"\n",
                "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
                "\n",
                "        if self.vectordb:\n",
                "            retriever = self.vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
                "            qa_chain = RetrievalQA.from_chain_type(\n",
                "                llm=llm,\n",
                "                chain_type=\"stuff\",\n",
                "                retriever=retriever,\n",
                "                return_source_documents=True,\n",
                "                chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
                "            )\n",
                "            return qa_chain\n",
                "        else:\n",
                "            print(\"No knowledge base found. Running in pure LLM mode.\")\n",
                "            return None\n",
                "            \n",
                "print(\"‚úÖ MedicalRAG class defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 5: Model Loading ---\n",
                "def load_model():\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()\n",
                "    \n",
                "    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        base_model = AutoModelForCausalLM.from_pretrained(\n",
                "            BASE_MODEL_NAME,\n",
                "            quantization_config=bnb_config,\n",
                "            device_map=\"auto\"\n",
                "        )\n",
                "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading model {BASE_MODEL_NAME}: {e}\")\n",
                "        print(\"Falling back to a smaller model for demo purposes...\")\n",
                "        # Fallback to a smaller model if the big one fails or needs access\n",
                "        fallback_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "        base_model = AutoModelForCausalLM.from_pretrained(\n",
                "             fallback_model,\n",
                "             quantization_config=bnb_config, \n",
                "             device_map=\"auto\"\n",
                "        )\n",
                "        tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n",
                "    \n",
                "    if os.path.exists(ADAPTER_NAME):\n",
                "        print(f\"Loading Adapter: {ADAPTER_NAME}\")\n",
                "        model = PeftModel.from_pretrained(base_model, ADAPTER_NAME)\n",
                "    else:\n",
                "        print(\"Using Base Model (No Adapter found).\")\n",
                "        model = base_model\n",
                "        \n",
                "    return model, tokenizer\n",
                "\n",
                "# Initialize Global Components\n",
                "print(\"Initializing System...\")\n",
                "model, tokenizer = load_model()\n",
                "rag_system = MedicalRAG()\n",
                "qa_chain = rag_system.setup_rag_pipeline(model, tokenizer)\n",
                "print(\"‚úÖ System Initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Step 6: Server Startup ---\n",
                "app = FastAPI(title=\"Medical RAG GPU API\")\n",
                "\n",
                "class QueryRequest(BaseModel):\n",
                "    message: str\n",
                "\n",
                "@app.get(\"/\")\n",
                "def home():\n",
                "    return {\"status\": \"online\", \"model\": BASE_MODEL_NAME}\n",
                "\n",
                "@app.post(\"/query\")\n",
                "def query_model(req: QueryRequest):\n",
                "    if qa_chain:\n",
                "        # RAG Mode\n",
                "        try:\n",
                "            res = qa_chain.invoke({\"query\": req.message})\n",
                "            return {\"answer\": res[\"result\"], \"source_documents\": [d.page_content[:200] for d in res.get(\"source_documents\", [])]}\n",
                "        except Exception as e:\n",
                "             return {\"error\": str(e)}\n",
                "    else:\n",
                "        # Pure LLM Mode\n",
                "        inputs = tokenizer(req.message, return_tensors=\"pt\").to(model.device)\n",
                "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
                "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        return {\"answer\": answer}\n",
                "\n",
                "@app.post(\"/ingest\")\n",
                "async def ingest_file(file: UploadFile = File(...)):\n",
                "    file_location = f\"./{file.filename}\"\n",
                "    with open(file_location, \"wb+\") as file_object:\n",
                "        file_object.write(file.file.read())\n",
                "    \n",
                "    rag_system.ingest_documents([file_location])\n",
                "    \n",
                "    # Reload the pipeline to include new data\n",
                "    global qa_chain\n",
                "    qa_chain = rag_system.setup_rag_pipeline(model, tokenizer)\n",
                "    \n",
                "    return {\"message\": f\"Successfully ingested {file.filename}\"}\n",
                "\n",
                "# Connect Ngrok\n",
                "if NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n",
                "    public_url = ngrok.connect(8000).public_url\n",
                "    print(f\"\\nüöÄ PUBLIC API URL: {public_url}\\n\")\n",
                "    print(f\"üëâ Copy this URL for your local client.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è NGROK_AUTH_TOKEN not set. Remote access will not work.\")\n",
                "\n",
                "async def run_server():\n",
                "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
                "    server = uvicorn.Server(config)\n",
                "    await server.serve()\n",
                "\n",
                "# Run the server\n",
                "print(\"Starting Server...\")\n",
                "# In notebook, we use the existing event loop\n",
                "if __name__ == \"__main__\":\n",
                "    try:\n",
                "        loop = asyncio.get_event_loop()\n",
                "        if loop.is_running():\n",
                "             # When running in a cell with an existing loop (normal for Colab)\n",
                "            asyncio.create_task(run_server())\n",
                "        else:\n",
                "            asyncio.run(run_server())\n",
                "    except RuntimeError:\n",
                "         await run_server()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}