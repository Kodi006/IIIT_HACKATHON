{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Information Assistant (Colab Edition)\n",
        "\n",
        "This notebook acts as a standalone, secure medical information assistant.\n",
        "\n",
        "**Features:**\n",
        "- Runs entirely on Google Colab (Free Tier T4 GPU supported)\n",
        "- Uses an open-source model (Llama-3-8B-Instruct or similar 4-bit quantized)\n",
        "- Strictly follows safety guidelines: No diagnosis, No prescriptions.\n",
        "- No external API keys required.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Connect to a GPU runtime (Runtime > Change runtime type > T4 GPU).\n",
        "2.  Run all cells in order."
      ],
      "metadata": {
        "id": "intro_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies\n",
        "We install `unsloth` for faster 4-bit inference and other necessary libraries."
      ],
      "metadata": {
        "id": "deps_text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "install_deps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the Model\n",
        "We use `unsloth/llama-3-8b-Instruct-bnb-4bit`. This model is highly optimized, small enough for Colab's T4 GPU, and excellent at following instructions."
      ],
      "metadata": {
        "id": "load_model_text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "load_model_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configure the Assistant\n",
        "Here we define the system prompt that ensures the model behaves safely and professionally."
      ],
      "metadata": {
        "id": "config_assistant_text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Define the System Prompt strictly following the user's constraints\n",
        "medical_system_prompt = \"\"\"\n",
        "You are a medical information assistant. Your purpose is to provide general, educational health information.\n",
        "\n",
        "STRICT RULES:\n",
        "- Do NOT diagnose diseases.\n",
        "- Do NOT prescribe medicines or treatments.\n",
        "- Do NOT recommend specific drugs (no antibiotics, steroids, or prescriptions).\n",
        "- Do NOT ask follow-up questions.\n",
        "- Do NOT role-play as a doctor.\n",
        "- Do NOT include legal disclaimers.\n",
        "\n",
        "Answer style:\n",
        "- Use simple, clear language.\n",
        "- Respond in 3â€“5 short bullet points.\n",
        "- Focus on general care, lifestyle habits, hygiene, rest, hydration, and symptom monitoring.\n",
        "- If medical certainty is not possible, begin with: \"General health advice includes:\"\n",
        "\n",
        "Safety behavior:\n",
        "- If input is vague, politely request a clearer question.\n",
        "- If question exceeds general education, provide high-level guidance only.\n",
        "- Never hallucinate medical facts.\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(user_input):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": medical_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(inputs, max_new_tokens = 512, use_cache = True)\n",
        "    response = tokenizer.batch_decode(outputs)\n",
        "    \n",
        "    # Parse the response to get only the assistant's part\n",
        "    # Llama-3 format usually puts answer after header. We'll do a simple split if needed,\n",
        "    return tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "define_prompt_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Medical Assistant\n",
        "Run the cell below to start chatting."
      ],
      "metadata": {
        "id": "run_chat_text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"medi-bot: Hello! I am your AI medical assistant. I can help with general health questions.\")\n",
        "print(\"          (Type 'exit' to stop)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"medi-bot: Stay healthy! Goodbye.\")\n",
        "            break\n",
        "            \n",
        "        if not user_input.strip():\n",
        "            continue\n",
        "\n",
        "        print(\"medi-bot:\", end=\" \")\n",
        "        response = generate_response(user_input)\n",
        "        print(response)\n",
        "        print(\"-\" * 60)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExiting...\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "chat_loop"
      }
    }
  ]
}
